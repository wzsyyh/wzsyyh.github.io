---
title: 'Auto-Slides: An Interactive Multi-Agent System for Creating and Customizing Research Presentations'

# Authors
# If you created a profile for a user (e.g. the default `admin` user), write the username (folder name) here
# and it will be replaced with their full name and linked to their profile.
authors:
  - admin
  - Wenjia Jiang
  - Yang Wang
  - Yiwei Wang
  - Chi Zhang

# Author notes (optional)
author_notes:
  - 'First author'
  - 'Equal contribution'
  - 'Equal contribution'
  - 'Equal contribution'
  - 'Corresponding author'

date: '2025-09-14T00:00:00Z'

# Schedule page publish date (NOT publication's date).
publishDate: '2025-09-14T00:00:00Z'

# Publication type.
# Accepts a single type but formatted as a YAML list (for Hugo requirements).
# Enter a publication type from the CSL standard.
publication_types: ['paper-conference']

# Publication name and optional abbreviated publication name.
publication: In *arXiv preprint*
publication_short: In *arXiv*

abstract: |
  The rapid progress of large language models (LLMs) has opened new opportunities for education. While learners can interact with academic papers through LLM-powered dialogue, limitations still exist: absence of structured organization and high text reliance can impede systematic understanding and engagement with complex concepts. To address these challenges, we propose Auto-Slides, an LLM-driven system that converts research papers into pedagogically structured, multimodal slides (e.g., diagrams and tables). Drawing on cognitive science, it creates a presentation-oriented narrative and allows iterative refinement via an interactive editor, in order to match learners' knowledge level and goals. Auto-Slides further incorporates verification and knowledge retrieval mechanisms to ensure accuracy and contextual completeness. Through extensive user studies, Auto-Slides enhances learners' comprehension and engagement compared to conventional LLM-based reading. Our contributions lie in designing a multi-agent framework for transforming academic papers into pedagogically optimized slides and introducing interactive customization for personalized learning.

# Summary. An optional shortened abstract.
summary: |
  We propose Auto-Slides, an LLM-driven multi-agent system that converts research papers into pedagogically structured, multimodal slides with interactive customization capabilities.

tags:
  - AI Agent
  - Multi-Agent Systems
  - Educational Technology
  - Human-Computer Interaction

# Display this page in the Featured widget?
featured: true

# Standard identifiers for auto-linking
hugoblox:
  ids:
    arxiv: 2509.11062

# Custom links
links:
  - type: pdf
    url: https://arxiv.org/pdf/2509.11062
  - type: code
    url: https://github.com/Westlake-AGI-Lab/Auto-Slides
  - type: project
    url: https://auto-slides.github.io/
  - type: arxiv
    url: https://arxiv.org/abs/2509.11062

# Featured image
# To use, add an image named `featured.jpg/png` to your page's folder.
image:
  caption: 'Auto-Slides: An overview of the multi-agent system for academic presentation generation'
  focal_point: ''
  preview_only: false

# Associated Projects (optional).
#   Associate this publication with one or more of your projects.
#   Simply enter your project's folder or file name without extension.
#   E.g. `internal-project` references `content/project/internal-project/index.md`.
#   Otherwise, set `projects: []`.
projects: []

# Slides (optional).
#   Associate this publication with Markdown slides.
#   Simply enter your slide deck's filename without extension.
#   E.g. `slides: "example"` references `content/slides/example/index.md`.
#   Otherwise, set `slides: ""`.
slides: ""
---

## Project Links

- **Project Homepage**: [auto-slides.github.io](https://auto-slides.github.io/)
- **arXiv Paper**: [arXiv:2509.11062](https://arxiv.org/abs/2509.11062)
- **GitHub Code**: [Westlake-AGI-Lab/Auto-Slides](https://github.com/Westlake-AGI-Lab/Auto-Slides)
